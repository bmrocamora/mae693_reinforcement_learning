{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAE693 Reinforcement Learning and Control\n",
    "## Homework 1\n",
    "### Bernardo Martinez Rocamora Junior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to import our libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's start our environment and test a random state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define Environment\n",
    "env = gym.make('Taxi-v3')\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let us define our two Policy Iteration functions: Policy Evaluation and Policy Improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Policy Evaluation ---\n",
    "def policy_evaluation (env, policy, gamma, threshold, max_iterations):\n",
    "\n",
    "    # Start evaluation by allocating value for all states as zeros\n",
    "    value_matrix = np.zeros(env.env.nS)\n",
    "\n",
    "    # Loop\n",
    "    for iteration in range(max_iterations):\n",
    "\n",
    "        # Initialize the maximum change of value across all states as zero\n",
    "        max_value_change = 0\n",
    "\n",
    "        # Find value for each state\n",
    "        for state in range(env.env.nS):\n",
    "\n",
    "            # Initialize value as zero\n",
    "            state_value = 0\n",
    "            \n",
    "            # Sum cost+gamma*value(state+action) over all states, considering their probabilities\n",
    "            for action in range(env.env.nA): \n",
    "                prob_action_given_state = policy[state][action]\n",
    "                (prob_transition, next_state, reward, done) = env.env.P[state][action][0]\n",
    "                state_value += prob_action_given_state * prob_transition * (reward + gamma * value_matrix[next_state])\n",
    "\n",
    "            # Calculate the change of the value of this state\n",
    "            value_change = np.abs(state_value - value_matrix[state])\n",
    "\n",
    "            # Check if value change is greater than maximum value change ever recorded. If yes, substitute\n",
    "            if (max_value_change < value_change):\n",
    "                max_value_change = value_change\n",
    "\n",
    "            # Update value for this state\n",
    "            value_matrix[state] = state_value\n",
    "\n",
    "        # Check if the maximum value change across all state is lesser than a certain threshold. If yes, return\n",
    "        if (max_value_change < threshold): \n",
    "            break\n",
    "\n",
    "    return value_matrix, iteration\n",
    "\n",
    "# ---- Policy Improvement\n",
    "def policy_improvement (env, policy, value_matrix, gamma):\n",
    "    # Initialize flags\n",
    "    found_optimum = False\n",
    "    policy_stable = True\n",
    "\n",
    "    # Run improvement for all states\n",
    "    for state in range(env.env.nS):\n",
    "        # Check what was the best action using old policy for this state\n",
    "        prior_best_action = np.argmax(policy[state])\n",
    "\n",
    "        # Using the new value matrix, store values across all actions for this state\n",
    "        action_values = np.zeros(env.env.nA)\n",
    "        for action in range(env.env.nA): \n",
    "            (prob_transition, next_state, reward, done) = env.env.P[state][action][0]\n",
    "            action_values[action] += prob_transition * (reward + gamma * value_matrix[next_state])\n",
    "\n",
    "        # Check the new best action from the stored values\n",
    "        best_action = np.argmax(action_values)\n",
    "\n",
    "        # If the old best action and new best action are different, change the flag to false\n",
    "        # so that the algorithm knows that the optimal policy has not been found yet\n",
    "        if prior_best_action != best_action:\n",
    "            policy_stable = False\n",
    "\n",
    "        # Update the policy with the new best action\n",
    "        policy[state] = np.eye(env.env.nA)[best_action]\n",
    "\n",
    "    # Return that the optimum was found if all old best actions were the same as the new\n",
    "    if policy_stable:\n",
    "        found_optimum = True\n",
    "        return policy, found_optimum    \n",
    "\n",
    "    return policy, found_optimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define a policy to start with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a policy for which all actions have the same probability of happening \n",
    "policy = np.ones([env.env.nS, env.env.nA]) / env.env.nA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we iterate and train our policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    # Evaluate policy\n",
    "    V,iteration = policy_evaluation (env, policy, 0.95, 0.000001, 10000)\n",
    "\n",
    "    # Improve policy\n",
    "    policy, found_optimum = policy_improvement (env, policy, V, 0.95)\n",
    "\n",
    "    # Check finishing condition\n",
    "    if (found_optimum == True):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's test the policy we found on a random enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | :\u001b[43m \u001b[0m:\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :\u001b[42mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : :\u001b[42m_\u001b[0m|\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : :\u001b[42m_\u001b[0m|\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : :\u001b[42m_\u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : :\u001b[42m_\u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| :\u001b[42m_\u001b[0m: : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| :\u001b[42m_\u001b[0m| : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m:\u001b[42m_\u001b[0m| : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35m\u001b[42mR\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "Mission complete!\n"
     ]
    }
   ],
   "source": [
    "# Start simulation with random starting configuration\n",
    "state = env.reset()\n",
    "\n",
    "while True:\n",
    "    # Step the simulation\n",
    "    state, reward, done, info = env.step(np.argmax(policy[state])) \n",
    "\n",
    "    # Update the state\n",
    "    env.env.s = state\n",
    "\n",
    "    # Render the simulation\n",
    "    env.render()\n",
    "\n",
    "    # Check finish condition\n",
    "    if (reward == 20):\n",
    "        break\n",
    "\n",
    "print(\"Mission complete!\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
