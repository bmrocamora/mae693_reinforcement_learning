{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "HW2_BernardoMartinez.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bmrocamora/mae693_rlc/blob/main/hw2/HW2_BernardoMartinez.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoCYqgjIbl77"
      },
      "source": [
        "# Homework 2\n",
        "\n",
        "### Due Date: Friday, March 20\n",
        "\n",
        "### Policy Gradient\n",
        "\n",
        "In this assignment, we will implement vanilla policy gradient algorithm (REINFORCE) covered in the lecture. You will work on i) a function approximator, ii) computing action, iii) collecting samples, iV) training the agent, V) plotting the resutls. \n",
        "\n",
        "\n",
        "***Complete the missing operations and test your implemented algorithm on the Gym environment.***\n",
        "\n",
        "***Software requirements:***\n",
        "- Python >= 3.6\n",
        "- Tensorflow version <= 1.15.3 (1.X version)\n",
        "- OpenAI Gym\n",
        "\n",
        "- Training the agent (policy) can take long time. It is recomended to start solving the problems earlier.\n",
        "\n",
        "- Save any plots you generated in this notebook. The grade will be given based on the plots you showed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uvw7Fllubl78"
      },
      "source": [
        "Make sure the packages you installed meet the requirements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "ZOwz00H2bwga",
        "outputId": "9d542290-3cef-4960-d707-813042334b1a"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.15.2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3b16D3_pbl79",
        "outputId": "5c172a0e-c2f2-43f1-f17f-81adc7e7ceb1"
      },
      "source": [
        "import gym\n",
        "gym.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0.17.3'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eFzxzFubl7-"
      },
      "source": [
        "## 1.1 Tensorflow Implementation\n",
        "\n",
        "We will be implementing policy gradient algorithm using Tensorflow 1.X., which simply updates the parameters of policy from obtaining gradient estimates. The core of policy gradient is to design a function approximator, computing actions, collecting samples, and training the policy. In the below cell, you are encouraged to fill in the components that are missing. ***Your tasks*** are \n",
        "\n",
        "1. Complete the 'create_model' method to output the mean value for diagonal Guassian policy. Covariance is already defined in the model, so focus on creating neural network model.\n",
        "\n",
        "2. Complete the 'action_op' method to calculate and return the actions for diagonal Gaussian policy. The applied action should be $\\pi(s) = \\pi_{\\text{mean}}(s) + exp(logstd) * \\mathcal{N}(0,1)$\n",
        "\n",
        "***Hints***:\n",
        "- Some useful tensorflow classes and methods include: 'tf.exp', 'tf.random_normal'\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxroA-ATbl7-"
      },
      "source": [
        "***IF you are using MAC, please run below box***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBBFowWJbl7_"
      },
      "source": [
        "# import os\n",
        "# MAC user only\n",
        "# os.environ['KMP_DUPLICATE_LIB_OK']='True'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUHbcKSfbl7_"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "class PolicyOpt(object):\n",
        "\n",
        "    def __init__(self, env, linear=False, stochastic=True, hidden_size=16, nonlinearity=tf.nn.relu):\n",
        "        \"\"\"Instantiate the policy iteration class.\n",
        "\n",
        "        This initializes the policy optimization with a set of trainable \n",
        "        parameters, and creates a tensorflow session.\n",
        "\n",
        "        Attributes\n",
        "        ----------\n",
        "        env : gym.Env\n",
        "            the environment that the policy will be trained on\n",
        "        linear : bool, optional\n",
        "            specifies whether to use a linear or neural network \n",
        "            policy, defaults to False (i.e. Fully-Connected-Neural-Network)\n",
        "        stochastic : bool, optional\n",
        "            specifies whether to use a stochastic or deterministic \n",
        "            policy, defaults to True\n",
        "        hidden_size : list of int, optional\n",
        "            list of hidden layers, with each value corresponding \n",
        "            to the number of nodes in that layer \n",
        "        nonlinearity : tf.nn.*\n",
        "            activation nonlinearity\n",
        "        \"\"\"\n",
        "        \n",
        "        # clear computation graph\n",
        "        tf.reset_default_graph()\n",
        "        \n",
        "        # set a random seed\n",
        "        tf.set_random_seed(1234)\n",
        "        \n",
        "        # start a tensorflow session\n",
        "        self.sess = tf.Session()\n",
        "        \n",
        "        # environment to train on\n",
        "        self.env = env\n",
        "        \n",
        "        # number of elements in the action space\n",
        "        self.ac_dim = env.action_space.shape[0]\n",
        "        \n",
        "        # number of elements in the observation space\n",
        "        self.obs_dim = env.observation_space.shape[0]\n",
        "\n",
        "        # actions placeholder\n",
        "        self.a_t_ph = tf.placeholder(dtype=tf.float32, \n",
        "                                     shape=[None, self.ac_dim])\n",
        "        # state placeholder\n",
        "        self.s_t_ph = tf.placeholder(dtype=tf.float32, \n",
        "                                     shape=[None, self.obs_dim])\n",
        "        # expected reward placeholder\n",
        "        self.rew_ph = tf.placeholder(dtype=tf.float32, \n",
        "                                     shape=[None])\n",
        "\n",
        "        # specifies whether the policy is stochastic\n",
        "        self.stochastic = stochastic\n",
        "\n",
        "        # policy that the agent executes during training/testing\n",
        "        self.policy = self.create_model(\n",
        "            args={\n",
        "                \"num_actions\": self.ac_dim,\n",
        "                \"hidden_size\": hidden_size,\n",
        "                \"linear\": linear,\n",
        "                \"nonlinearity\": nonlinearity,\n",
        "                \"stochastic\": stochastic,\n",
        "                \"scope\": \"policy\",\n",
        "            }\n",
        "        )\n",
        "        \n",
        "        # define symbolic action\n",
        "        self.symbolic_action = self.action_op()\n",
        "\n",
        "        # initialize all variables\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        # create saver to save model variables\n",
        "        self.saver = tf.train.Saver()\n",
        "\n",
        "    def create_model(self, args):\n",
        "        \"\"\"Create a model for your policy or other components.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        args : dict\n",
        "            model-specific arguments, with keys:\n",
        "              - \"stochastic\": True by default\n",
        "              - \"hidden_size\": Number of neurons in hidden layer\n",
        "              - \"num_actions\" number of output actions\n",
        "              - \"scope\": scope of the model\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        tf.Variable\n",
        "            mean actions of the policy\n",
        "        tf.Variable \n",
        "            logstd of the policy actions\n",
        "        \"\"\"\n",
        "\n",
        "#################### Build Your Neural Network Here! ####################\n",
        "        # Let's build a three hidden layer model.        \n",
        "        \n",
        "        # INPUT  --- LAYER1 ---- LAYER2 ---- LAYER 3 ---- OUTPUT\n",
        "        input_mean = self.s_t_ph\n",
        "\n",
        "        first_layer = tf.layers.dense(input_mean, args[\"hidden_size\"], activation=args[\"nonlinearity\"])\n",
        "        second_layer = tf.layers.dense(first_layer, args[\"hidden_size\"], activation=args[\"nonlinearity\"])\n",
        "        third_layer = tf.layers.dense(second_layer, args[\"hidden_size\"], activation=args[\"nonlinearity\"])\n",
        "\n",
        "        output_mean = tf.layers.dense(third_layer, args[\"num_actions\"])\n",
        "        \n",
        "##########################################################################        \n",
        "\n",
        "        if args[\"stochastic\"]:\n",
        "            output_logstd =  tf.get_variable(name=\"action_logstd\",shape=[self.ac_dim],trainable=True)\n",
        "        else:\n",
        "            output_logstd = None\n",
        "\n",
        "        return output_mean, output_logstd\n",
        "    \n",
        "    def action_op(self):\n",
        "        \"\"\"\n",
        "        Create a symbolic expression that will be used to compute actions from observations.\n",
        "\n",
        "        When the policy is stochastic, the action follows \n",
        "\n",
        "            a_t = output_mean + exp(output_logstd) * z; z ~ N(0,1)\n",
        "        \"\"\"\n",
        "        if self.stochastic:\n",
        "            output_mean, output_logstd = self.policy\n",
        "\n",
        "            #################### Implement a stochastic policy here ####################        \n",
        "            # Implement a stochastic version of computing actions.       #\n",
        "            #                                                            #\n",
        "            # The action in a stochastic policy represented by           #\n",
        "            # a diagonal Gaussian distribution with mean \"M\" and log     #\n",
        "            # standard deviation \"logstd\" is computed as follows:        #\n",
        "            #                                                            #\n",
        "            #     a = M + exp(logstd) * z                                #\n",
        "            #                                                            #\n",
        "            # where z is a random normal value, i.e. z ~ N(0,1)          #\n",
        "            #                                                            #\n",
        "            # In order to generate numbers from a normal distribution,   #\n",
        "            # use the `tf.random_normal` function.                       #\n",
        "            ############################################################################ \n",
        "            symbolic_action = output_mean + tf.math.exp(output_logstd) * tf.random_normal([1], 0, 1,  tf.float32, seed=1)\n",
        "            \n",
        "        else:\n",
        "            symbolic_action, _ = self.policy\n",
        "        \n",
        "        return symbolic_action\n",
        "\n",
        "    def compute_action(self, obs):\n",
        "        \"\"\"Returns a list of actions for a given observation.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        obs : np.ndarray\n",
        "            observations\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        np.ndarray\n",
        "            actions by the policy for a given observation\n",
        "        \"\"\"\n",
        "        return self.sess.run(self.symbolic_action,feed_dict={self.s_t_ph: obs})\n",
        "\n",
        "    def rollout(self, s_mean=None, s_std=None):\n",
        "        \"\"\"Collect samples from one rollout of the policy.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        dict\n",
        "            dictionary containing trajectory information for the rollout,\n",
        "            specifically containing keys for \"state\", \"action\", \"next_state\", \"reward\", and \"done\"\n",
        "        \"\"\"\n",
        "        states = []\n",
        "        next_states = []\n",
        "        actions = []\n",
        "        rewards = []\n",
        "        dones = []\n",
        "\n",
        "        # start a new rollout by re-setting the environment and collecting the initial state\n",
        "        state =  self.env.reset()\n",
        "\n",
        "        steps = 0\n",
        "        while True:\n",
        "            steps += 1\n",
        "\n",
        "            # compute the action given the state\n",
        "            if s_mean is not None and s_std is not None:\n",
        "                action = self.compute_action([(state - s_mean) / s_std])\n",
        "            else:\n",
        "                action = self.compute_action([state])\n",
        "            action = action[0]\n",
        "\n",
        "            # advance the environment once and collect the next state, reward, done, and info parameters from the environment\n",
        "            next_state, reward, done, info =  self.env.step(action)\n",
        "\n",
        "            # add to the samples list\n",
        "            states.append(state)\n",
        "            actions.append(action)\n",
        "            next_states.append(next_state)\n",
        "            rewards.append(reward)\n",
        "            dones.append(done)\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "            # if the environment returns a True for the done parameter,\n",
        "            # end the rollout before the time horizon is met\n",
        "            if done or steps > env._max_episode_steps:\n",
        "                break\n",
        "\n",
        "        # create the output trajectory\n",
        "        trajectory = {\"state\": np.array(states, dtype=np.float32),\n",
        "                      \"reward\": np.array(rewards, dtype=np.float32),\n",
        "                      \"action\": np.array(actions, dtype=np.float32),\n",
        "                      \"next_state\": np.array(next_states, dtype=np.float32),\n",
        "                      \"done\": np.array(dones, dtype=np.float32)}\n",
        "\n",
        "        return trajectory\n",
        "\n",
        "    def train(self, args):\n",
        "        \"\"\"Abstract training method.\n",
        "\n",
        "        This method will be filled in by algorithm-specific\n",
        "        training operations in subsequent problems.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        args : dict\n",
        "            algorithm-specific hyperparameters\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkCNlYjwbl8C"
      },
      "source": [
        "## 1.2 Tensorflow Interpretation\n",
        "\n",
        "In order to test your implementation of the **stochastic policy**, run the below cell. The task is to interpret the code you implemented in previous section. If you implement correctly, you can see the value_1 and value_2.\n",
        "\n",
        "***Question: How do you interpret value_1 and value_2 below cell?***\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8H21YoJbl8F",
        "outputId": "0290f865-ab5c-4613-a6da-cb66dde7ba9b"
      },
      "source": [
        "import gym\n",
        "TEST_ENV = gym.make(\"Pendulum-v0\")\n",
        "\n",
        "alg = PolicyOpt(TEST_ENV, linear=False)\n",
        "input_1 = [[0, 1, 2]]\n",
        "value_1 = alg.sess.run(alg.policy[0], feed_dict={alg.s_t_ph: input_1})\n",
        "value_2 = alg.compute_action(input_1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-f7caddaf63e0>:108: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VepvmRMpbl8F",
        "outputId": "0ed7be53-dac7-4e6e-ddd1-c5c5f9ab6103"
      },
      "source": [
        "value_1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.17123741]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcZC9fh7bl8G"
      },
      "source": [
        "Answer: This is the value returned when the tf.Session.run() method runs one \"step\" of TensorFlow computation. We are predetermining some input values [0, 1, 2] and getting the action value for the starting policy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cq2u90cbl8G",
        "outputId": "aca58327-cf23-4d53-d45c-d3ae9842d2eb"
      },
      "source": [
        "value_2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.01477186]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXgnIA_2bl8G"
      },
      "source": [
        " Answer: This value is the action computed for the same input values ([0, 1, 2]), but now using the self.symbolic_action function defined in the PolicyOpt class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XPiGmvwbl8G"
      },
      "source": [
        "## 1.3 Implement Policy Gradient\n",
        "\n",
        "In this section, we will implement REINFORCE algorithm presented in the lecture. As a review, the objective is optimize the parameters $\\theta$ of some policy $\\pi_\\theta$ so that the expected return\n",
        "\n",
        "\\begin{equation}\n",
        "J(\\theta) = \\mathbb{E} \\bigg\\{ \\sum_{t=0}^T \\gamma^t r(s_{t},a_{t}) \\bigg\\}\n",
        "\\end{equation}\n",
        "\n",
        "is optimized. In this algorithm, this is done by calculating the gradient $\\nabla_\\theta J$ and applying a gradient descent method to find a better policy.\n",
        "\n",
        "\\begin{equation}\n",
        "\\theta ' = \\theta + \\alpha \\nabla_\\theta J(\\theta)\n",
        "\\end{equation}\n",
        "\n",
        "In the lecture, we derive how we compute $\\nabla_{\\theta} J(\\theta)$. We can rewrite our policy gradient as:\n",
        "\n",
        "\\begin{equation}\n",
        "\\nabla_\\theta J (\\theta) \\approx \\frac{1}{N} \\sum_{i=0}^{N} \\bigg( \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta (a_{it} | s_{it}) \\bigg) \\bigg( \\sum_{t=0}^T \\gamma^{t}r_i(t) \\bigg)\n",
        "\\end{equation}\n",
        "\n",
        "Finally, taking into account the causality principle discussed in class, we are able to simplifiy the gradient estimate such as:\n",
        "\n",
        "\\begin{equation}\n",
        "\\nabla_\\theta J (\\theta) \\approx \\frac{1}{N} \\sum_{i=0}^{N} \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta (a_{it} | s_{it}) \\sum_{t'=t}^T \\gamma^{t'-t}r_i(t')\n",
        "\\end{equation}\n",
        "\n",
        "You will be implementing final expression in this assignment!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rK8t5Maubl8H"
      },
      "source": [
        "The process of REINFOCE algorithm follows:\n",
        "\n",
        "1. Collect samples from current policy $\\pi_\\theta(s)$ by executing rollouts of the environment.\n",
        "2. Calculate an estimate for the expected return at state $s_t$. \n",
        "3. Compute the log-likelihood of each action that was performed by the policy at every given step.\n",
        "4. Estimate the gradient and update the parameters of policy using gradient-based technique.\n",
        "5. Repeat steps 1-4 for a number of training iterations.\n",
        "\n",
        "***Your task*** is to fill out the skeleton code for REINFORCE algorithm,\n",
        "\n",
        "1. Complete the 'log_likelihoods' method to compute gradient of policy, $\\nabla_{\\theta}\\pi_{\\theta}$ for diagonal Guassian policy. \n",
        "\n",
        "2. Complete the 'compute_expected_return' method to calculate the reward-to-go, $\\sum_{t^{\\prime}=t}^{T}$. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Sj6rjHDbl8H"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "class REINFORCE(PolicyOpt):\n",
        "\n",
        "    def train(self, num_iterations=1000, steps_per_iteration=1000, learning_rate=int(1e-4), gamma=0.95, \n",
        "              **kwargs):\n",
        "        \"\"\"Perform the REINFORE training operation.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        num_iterations : int\n",
        "            number of training iterations\n",
        "        steps_per_iteration : int\n",
        "            number of individual samples collected every training iteration\n",
        "        learning_rate : float\n",
        "            optimizer learning rate\n",
        "        gamma : float\n",
        "            discount rate\n",
        "        kwargs : dict\n",
        "            additional arguments\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        list of float\n",
        "            average return per iteration\n",
        "        \"\"\"\n",
        "        # set the discount as an attribute\n",
        "        self.gamma = gamma\n",
        "        \n",
        "        # set the learning rate as an attribute\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # create a symbolic expression to compute the log-likelihoods \n",
        "        log_likelihoods = self.log_likelihoods()\n",
        "\n",
        "        # create a symbolic expression for updating the parameters of your policy\n",
        "        self.opt, self.opt_baseline = self.define_updates(log_likelihoods)\n",
        "        \n",
        "        # initialize all variables\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "\n",
        "\n",
        "        # average return per training iteration\n",
        "        ret_per_iteration = []\n",
        "        \n",
        "        samples = []\n",
        "        for i in range(num_iterations):\n",
        "            \n",
        "            # collect samples from the current policy\n",
        "            samples.clear()\n",
        "            steps_so_far = 0\n",
        "            while steps_so_far < steps_per_iteration:\n",
        "                new_samples = self.rollout()\n",
        "                steps_so_far += new_samples[\"action\"].shape[0]\n",
        "                samples.append(new_samples)\n",
        "\n",
        "            # compute the expected returns\n",
        "            v_s = self.compute_expected_return(samples)\n",
        "\n",
        "            # compute and apply the gradients\n",
        "            self.call_updates(log_likelihoods, samples, v_s, **kwargs)\n",
        "\n",
        "            # compute the average cumulative return per iteration\n",
        "            average_rew = np.mean([sum(s[\"reward\"]) for s in samples])\n",
        "\n",
        "            # display iteration statistics\n",
        "            print(\"Iteration {} return: {}\".format(i, average_rew))\n",
        "            ret_per_iteration.append(average_rew)\n",
        "\n",
        "        return ret_per_iteration\n",
        "\n",
        "    def log_likelihoods(self):\n",
        "        \"\"\"Create a tensorflow operation that computes the log-likelihood \n",
        "        of each performed action.\n",
        "        \"\"\"\n",
        "        \n",
        "        output_mean, output_logstd = self.policy\n",
        "\n",
        "        ##############################################################\n",
        "        # Create a tf operation to compute the log-likelihood of     #\n",
        "        # each action that was performed by the policy               #\n",
        "        #                                                            #\n",
        "        # The log likelihood in the continuous case where the policy #\n",
        "        # is expressed by a multivariate gaussian can be computing   #\n",
        "        # using the tensorflow object:                               #\n",
        "        #                                                            #\n",
        "        #    p = tfp.distributions.MultivariateNormalDiag(           #\n",
        "        #        loc=...,                                            #\n",
        "        #        scale_diag=...,                                     #\n",
        "        #    )                                                       #\n",
        "        #                                                            #\n",
        "        # This method takes as input a mean (loc) and standard       #\n",
        "        # deviation (scale_diag), and then can be used to compute    #\n",
        "        # the log-likelihood of a variable as follows:               #\n",
        "        #                                                            #\n",
        "        #    log_likelihoods = p.log_prob(...)                       #\n",
        "        #                                                            #\n",
        "        # For this operation, you will want to use placeholders      #\n",
        "        # created in the __init__ method of problem 1.               #\n",
        "        ##############################################################\n",
        "\n",
        "        p = tfp.distributions.MultivariateNormalDiag(loc=output_mean,\n",
        "                                                     scale_diag=output_logstd)\n",
        "        log_likelihoods = p.log_prob(self.a_t_ph)\n",
        "\n",
        "        return log_likelihoods\n",
        "\n",
        "    def compute_expected_return(self, samples):\n",
        "        \"\"\"Compute the expected return from a given starting state.\n",
        "        This is done by using the reward-to-go method.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        rewards : list of list of float\n",
        "            a list of N trajectories, with each trajectory contain T \n",
        "            returns values (one for each step in the trajectory)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        list of float, or np.ndarray\n",
        "            expected returns for each step in each trajectory\n",
        "        \"\"\"\n",
        "        rewards = [s[\"reward\"] for s in samples]\n",
        "\n",
        "        ##############################################################\n",
        "        # Estimate the expected return from any given starting state #\n",
        "        # using the reward-to-go method.                             #\n",
        "        #                                                            #\n",
        "        # Using this method, the reward is estimated at every step   #\n",
        "        # of the trajectory as follows:                              #\n",
        "        #                                                            #\n",
        "        #   r = sum_{t'=t}^T gamma^(t'-t) * r_{t'}                   #\n",
        "        #                                                            #\n",
        "        # where T is the time horizon at t is the index of the       #\n",
        "        # current reward in the trajectory. For example, for a given #\n",
        "        # set of rewards r = [1,1,1,1] and discount rate gamma = 1,  #\n",
        "        # the expected reward-to-go would be:                        #\n",
        "        #                                                            #\n",
        "        #   v_s = [4, 3, 2, 1]                                       #\n",
        "        #                                                            #\n",
        "        # You will be able to test this in one of the cells below!   #\n",
        "        ##############################################################\n",
        "        \n",
        "        v_s = np.array([])\n",
        "        for reward in rewards:\n",
        "          T = len(reward)\n",
        "          for t in range(T):\n",
        "            v_s= np.append(v_s, np.dot(reward[t:],(self.gamma*np.ones([T-t])) ** np.arange(T-t)))\n",
        "        \n",
        "        return v_s\n",
        "\n",
        "    def define_updates(self, log_likelihoods):\n",
        "        \"\"\"Create a tensorflow operation to update the parameters of \n",
        "        your policy.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        log_likelihoods : tf.Operation\n",
        "            the symbolic expression you created to estimate the log \n",
        "            likelihood of a set of actions\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        tf.Operation\n",
        "            a tensorflow operation for computing and applying the \n",
        "            gradients to the parameters of the policy\n",
        "        None\n",
        "            the second component is used in problem 2.b, please ignore \n",
        "            for this problem\n",
        "        \"\"\"\n",
        "\n",
        "        loss = - tf.reduce_mean(tf.multiply(log_likelihoods, self.rew_ph))\n",
        "        opt = tf.train.AdamOptimizer(self.learning_rate).minimize(loss)\n",
        "\n",
        "        return opt, None\n",
        "\n",
        "    def call_updates(self, log_likelihoods, samples, v_s, **kwargs):\n",
        "        \"\"\"Apply the gradient update methods in a tensorflow session.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        log_likelihoods: tf.Operation\n",
        "            the symbolic expression you created to estimate the log \n",
        "            likelihood of a set of actions\n",
        "        samples : list of dict\n",
        "            a list of N trajectories, with each trajectory containing \n",
        "            a dictionary of trajectory data (see self.rollout)\n",
        "        v_s : list of float, or np.ndarray\n",
        "            the estimated expected returns from your\n",
        "            `comput_expected_return` function \n",
        "        kwargs : dict\n",
        "            additional arguments (used in question 3)\n",
        "        \"\"\"\n",
        "        # concatenate the states\n",
        "        states = np.concatenate([s[\"state\"] for s in samples])\n",
        "\n",
        "        # concatenate the actions\n",
        "        actions = np.concatenate([s[\"action\"] for s in samples])\n",
        "\n",
        "        # execute the optimization step\n",
        "        self.sess.run(self.opt, feed_dict={self.s_t_ph: states,\n",
        "                                           self.a_t_ph: actions,\n",
        "                                           self.rew_ph: v_s})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEfv8zDvbl8H"
      },
      "source": [
        "Check your 'log_likelihoods' method by running below cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55eYlYC2bl8H"
      },
      "source": [
        "alg = REINFORCE(TEST_ENV, stochastic=True)\n",
        "\n",
        "log_likelihoods = alg.log_likelihoods()\n",
        "\n",
        "# collect a sample output for a given input state\n",
        "input_s = [[0, 0, 0], [0, 1, 2], [1, 2, 3]]\n",
        "input_a = [[0], [1], [2]]\n",
        "\n",
        "# Check\n",
        "computed = alg.sess.run(log_likelihoods, feed_dict={alg.a_t_ph: input_a, alg.s_t_ph: input_s})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sicCpyeSbl8I"
      },
      "source": [
        "Test your 'compute_expected_return' by running below cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgZvTGZIbl8I",
        "outputId": "7341fe70-66eb-4621-f14e-76a89a1a02fe"
      },
      "source": [
        "# 1. Test the non-normalized case\n",
        "alg = REINFORCE(TEST_ENV, stochastic=True)\n",
        "alg.gamma = 1.0\n",
        "    \n",
        "input_1 = [{\"reward\": [1, 1, 1, 1]},\n",
        "           {\"reward\": [1, 1, 1, 1]}]\n",
        "vs_1 = alg.compute_expected_return(samples=input_1)\n",
        "ans_1 = np.array([4, 3, 2, 1, 4, 3, 2, 1])\n",
        "\n",
        "if np.linalg.norm(vs_1 - ans_1) < 1e-3:\n",
        "    print('Great job!')\n",
        "else:\n",
        "    print('Check your implementation (compute_expected_return)')\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Great job!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSHKaXW_bl8I"
      },
      "source": [
        "## 1.4 Testing your algorithm\n",
        "\n",
        "When you are ready, test your policy gradient algorithms on the *Pendulum-v0* environment in the cell below. *Pendulum-v0* environment is similar to *off-shore wind power*, the goal here is to maintain the Pendulum is upright using control input. The best policy should get around -200 scores. ***Your task*** is to run your REINFORCE algorithm and plot the result!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znyn1oXTbl8I",
        "outputId": "557fbf96-516e-4d7f-d9f1-256417af9f06"
      },
      "source": [
        "# set this number as 1 for testing your algorithm, and 3 for plotting\n",
        "NUM_TRIALS = 3\n",
        "\n",
        "# ===========================================================================\n",
        "# Do not modify below line\n",
        "# ===========================================================================\n",
        "\n",
        "# we will test the algorithms on the Pendulum-v0 gym environment\n",
        "import gym\n",
        "env = gym.make(\"Pendulum-v0\")\n",
        "\n",
        "# train on the REINFORCE algorithm\n",
        "import numpy as np\n",
        "r = []\n",
        "for i in range(NUM_TRIALS):\n",
        "    print(\"\\n==== Training Run {} ====\".format(i))\n",
        "    alg = REINFORCE(env, stochastic=True)\n",
        "    res = alg.train(learning_rate=0.005, gamma=0.95, num_iterations=500, steps_per_iteration=15000)\n",
        "    r.append(np.array(res))\n",
        "    if i != 2: bjr\n",
        "    \n",
        "      alg = None\n",
        "\n",
        "# save results\n",
        "np.savetxt(\"InvertedPendulum_results.csv\", np.array(r), delimiter=\",\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "==== Training Run 0 ====\n",
            "Iteration 0 return: -1394.2431976047096\n",
            "Iteration 1 return: -1245.5705213834726\n",
            "Iteration 2 return: -1241.9994313087718\n",
            "Iteration 3 return: -1204.68080141888\n",
            "Iteration 4 return: -1188.616206532057\n",
            "Iteration 5 return: -1258.5294696929816\n",
            "Iteration 6 return: -1194.7140954982303\n",
            "Iteration 7 return: -1206.5109090436779\n",
            "Iteration 8 return: -1193.084497838576\n",
            "Iteration 9 return: -1189.6133407145367\n",
            "Iteration 10 return: -1131.4616217521625\n",
            "Iteration 11 return: -1131.6838630221312\n",
            "Iteration 12 return: -1165.7277322204868\n",
            "Iteration 13 return: -1188.6046445909574\n",
            "Iteration 14 return: -1165.382412312824\n",
            "Iteration 15 return: -1209.6385964032038\n",
            "Iteration 16 return: -1165.028492354988\n",
            "Iteration 17 return: -1219.304227383323\n",
            "Iteration 18 return: -1279.8382080262852\n",
            "Iteration 19 return: -1193.4294129156438\n",
            "Iteration 20 return: -1220.332830033449\n",
            "Iteration 21 return: -1207.3028733287938\n",
            "Iteration 22 return: -1180.2019866817693\n",
            "Iteration 23 return: -1054.6052885310119\n",
            "Iteration 24 return: -1090.128177746969\n",
            "Iteration 25 return: -1100.3256044845934\n",
            "Iteration 26 return: -1141.2736615953195\n",
            "Iteration 27 return: -1096.1980677442104\n",
            "Iteration 28 return: -1152.2159314938929\n",
            "Iteration 29 return: -1109.9689236681966\n",
            "Iteration 30 return: -1113.2800915614703\n",
            "Iteration 31 return: -1134.1990479973254\n",
            "Iteration 32 return: -1144.608047152362\n",
            "Iteration 33 return: -1112.89189098758\n",
            "Iteration 34 return: -1100.4598728900805\n",
            "Iteration 35 return: -1114.7717511756182\n",
            "Iteration 36 return: -1097.042067394197\n",
            "Iteration 37 return: -1093.9640280793515\n",
            "Iteration 38 return: -1088.2732807876853\n",
            "Iteration 39 return: -1061.8178964377287\n",
            "Iteration 40 return: -1095.796949000492\n",
            "Iteration 41 return: -1086.3916230094812\n",
            "Iteration 42 return: -1021.3428021269921\n",
            "Iteration 43 return: -1047.100514918266\n",
            "Iteration 44 return: -1034.9316220768044\n",
            "Iteration 45 return: -1099.9461461359822\n",
            "Iteration 46 return: -1038.9193393504347\n",
            "Iteration 47 return: -1036.438773316398\n",
            "Iteration 48 return: -1045.7050015658172\n",
            "Iteration 49 return: -1114.0695540337772\n",
            "Iteration 50 return: -1087.2912850150574\n",
            "Iteration 51 return: -1085.510978979885\n",
            "Iteration 52 return: -1023.41080400207\n",
            "Iteration 53 return: -1053.3447757077683\n",
            "Iteration 54 return: -1012.98727197845\n",
            "Iteration 55 return: -993.1464975744098\n",
            "Iteration 56 return: -1019.5783834747675\n",
            "Iteration 57 return: -969.3173413926297\n",
            "Iteration 58 return: -999.3740946317247\n",
            "Iteration 59 return: -1023.9079648099778\n",
            "Iteration 60 return: -1014.423428035255\n",
            "Iteration 61 return: -1024.3738910622849\n",
            "Iteration 62 return: -1023.4674702112749\n",
            "Iteration 63 return: -977.8275991565699\n",
            "Iteration 64 return: -986.2200607972014\n",
            "Iteration 65 return: -945.8514304076089\n",
            "Iteration 66 return: -993.6047737916085\n",
            "Iteration 67 return: -988.1119837959767\n",
            "Iteration 68 return: -1018.2008258176005\n",
            "Iteration 69 return: -933.1660374278092\n",
            "Iteration 70 return: -1042.2464278114703\n",
            "Iteration 71 return: -1113.8896071647791\n",
            "Iteration 72 return: -1088.740492344337\n",
            "Iteration 73 return: -971.6174295937238\n",
            "Iteration 74 return: -947.3640993720603\n",
            "Iteration 75 return: -975.3984150654954\n",
            "Iteration 76 return: -960.8445959841906\n",
            "Iteration 77 return: -968.173758395464\n",
            "Iteration 78 return: -965.1074909211385\n",
            "Iteration 79 return: -1019.639887592517\n",
            "Iteration 80 return: -1073.9931911453884\n",
            "Iteration 81 return: -916.633078402417\n",
            "Iteration 82 return: -977.9939753365879\n",
            "Iteration 83 return: -913.6871230026442\n",
            "Iteration 84 return: -930.6061476473332\n",
            "Iteration 85 return: -971.768656978528\n",
            "Iteration 86 return: -1037.7999491746414\n",
            "Iteration 87 return: -1081.475878238639\n",
            "Iteration 88 return: -1124.6043303570166\n",
            "Iteration 89 return: -1169.4268682568081\n",
            "Iteration 90 return: -1177.8216978709834\n",
            "Iteration 91 return: -1206.6143316054709\n",
            "Iteration 92 return: -1213.789638709044\n",
            "Iteration 93 return: -1231.8712991185673\n",
            "Iteration 94 return: -1249.5912087299116\n",
            "Iteration 95 return: -1256.2674471837934\n",
            "Iteration 96 return: -1272.9914134898781\n",
            "Iteration 97 return: -1263.3651193085434\n",
            "Iteration 98 return: -1281.3979257033518\n",
            "Iteration 99 return: -1279.9728339074243\n",
            "Iteration 100 return: -1294.6480651264\n",
            "Iteration 101 return: -1296.7689138017774\n",
            "Iteration 102 return: -1304.0786198828648\n",
            "Iteration 103 return: -1297.6134318330355\n",
            "Iteration 104 return: -1277.4208153684892\n",
            "Iteration 105 return: -1299.8248782921164\n",
            "Iteration 106 return: -1306.3996705034235\n",
            "Iteration 107 return: -1303.774829432154\n",
            "Iteration 108 return: -1300.3067325084378\n",
            "Iteration 109 return: -1299.5852893653637\n",
            "Iteration 110 return: -1302.504889540191\n",
            "Iteration 111 return: -1307.3121974839921\n",
            "Iteration 112 return: -1308.8898159228265\n",
            "Iteration 113 return: -1313.2796507795092\n",
            "Iteration 114 return: -1315.7689883476683\n",
            "Iteration 115 return: -1312.2291997545537\n",
            "Iteration 116 return: -1301.5520515165338\n",
            "Iteration 117 return: -1312.967236180647\n",
            "Iteration 118 return: -1316.6787510625397\n",
            "Iteration 119 return: -1297.2484451473629\n",
            "Iteration 120 return: -1302.1534955972636\n",
            "Iteration 121 return: -1301.1013451540493\n",
            "Iteration 122 return: -1310.2379929617466\n",
            "Iteration 123 return: -1302.1131083735825\n",
            "Iteration 124 return: -1313.6430559591763\n",
            "Iteration 125 return: -1309.3576001508907\n",
            "Iteration 126 return: -1311.0081065654506\n",
            "Iteration 127 return: -1302.7771456483224\n",
            "Iteration 128 return: -1316.7819423286726\n",
            "Iteration 129 return: -1292.810996720854\n",
            "Iteration 130 return: -1311.7673881015135\n",
            "Iteration 131 return: -1306.876838604187\n",
            "Iteration 132 return: -1306.2741952726656\n",
            "Iteration 133 return: -1310.8355377247465\n",
            "Iteration 134 return: -1306.9716692559762\n",
            "Iteration 135 return: -1300.8829157056534\n",
            "Iteration 136 return: -1308.316567148088\n",
            "Iteration 137 return: -1291.2284929328187\n",
            "Iteration 138 return: -1307.3576875294682\n",
            "Iteration 139 return: -1299.845657796914\n",
            "Iteration 140 return: -1298.9366488192045\n",
            "Iteration 141 return: -1305.672030327885\n",
            "Iteration 142 return: -1298.3396952253897\n",
            "Iteration 143 return: -1296.8564961058212\n",
            "Iteration 144 return: -1301.4538746390306\n",
            "Iteration 145 return: -1290.0635509104143\n",
            "Iteration 146 return: -1286.6293332317068\n",
            "Iteration 147 return: -1287.2043929340132\n",
            "Iteration 148 return: -1287.9009596385\n",
            "Iteration 149 return: -1290.070344967097\n",
            "Iteration 150 return: -1294.633712546186\n",
            "Iteration 151 return: -1289.6908357112482\n",
            "Iteration 152 return: -1287.1726357688383\n",
            "Iteration 153 return: -1280.2227471180183\n",
            "Iteration 154 return: -1282.1073041949592\n",
            "Iteration 155 return: -1280.2988197199256\n",
            "Iteration 156 return: -1270.1925207844336\n",
            "Iteration 157 return: -1278.6649461744998\n",
            "Iteration 158 return: -1277.8163215632178\n",
            "Iteration 159 return: -1265.8068729130086\n",
            "Iteration 160 return: -1290.7314437957527\n",
            "Iteration 161 return: -1274.3065847079913\n",
            "Iteration 162 return: -1274.2266045969166\n",
            "Iteration 163 return: -1265.5395419621777\n",
            "Iteration 164 return: -1267.9472589684651\n",
            "Iteration 165 return: -1266.8037992286652\n",
            "Iteration 166 return: -1258.2776619160052\n",
            "Iteration 167 return: -1259.064265607133\n",
            "Iteration 168 return: -1277.841805797702\n",
            "Iteration 169 return: -1270.4460605555248\n",
            "Iteration 170 return: -1258.2792642620213\n",
            "Iteration 171 return: -1255.7345129993776\n",
            "Iteration 172 return: -1263.7684086934057\n",
            "Iteration 173 return: -1256.465293882663\n",
            "Iteration 174 return: -1248.0109818208466\n",
            "Iteration 175 return: -1258.6356670256268\n",
            "Iteration 176 return: -1248.3401173928721\n",
            "Iteration 177 return: -1244.3201792322286\n",
            "Iteration 178 return: -1242.35808847297\n",
            "Iteration 179 return: -1228.440014128917\n",
            "Iteration 180 return: -1239.9083388971756\n",
            "Iteration 181 return: -1233.2702274678193\n",
            "Iteration 182 return: -1220.8429792957256\n",
            "Iteration 183 return: -1235.1573823324031\n",
            "Iteration 184 return: -1236.3380185018138\n",
            "Iteration 185 return: -1228.9244549272705\n",
            "Iteration 186 return: -1228.657830074765\n",
            "Iteration 187 return: -1204.2806047027589\n",
            "Iteration 188 return: -1200.6023302232784\n",
            "Iteration 189 return: -1208.0444197895588\n",
            "Iteration 190 return: -1203.1911508764294\n",
            "Iteration 191 return: -1217.2337218830494\n",
            "Iteration 192 return: -1204.9894720602222\n",
            "Iteration 193 return: -1215.4156323099153\n",
            "Iteration 194 return: -1194.4986703854245\n",
            "Iteration 195 return: -1212.5968690187049\n",
            "Iteration 196 return: -1188.851376560242\n",
            "Iteration 197 return: -1182.5556294305281\n",
            "Iteration 198 return: -1192.5685972997012\n",
            "Iteration 199 return: -1208.5303650219987\n",
            "Iteration 200 return: -1184.2196182749824\n",
            "Iteration 201 return: -1149.9602529835577\n",
            "Iteration 202 return: -1201.2659984026484\n",
            "Iteration 203 return: -1198.9513568184464\n",
            "Iteration 204 return: -1169.1976183416998\n",
            "Iteration 205 return: -1155.8646762779636\n",
            "Iteration 206 return: -1164.1835327218348\n",
            "Iteration 207 return: -1165.9430332076176\n",
            "Iteration 208 return: -1159.0051283782007\n",
            "Iteration 209 return: -1158.6492518311234\n",
            "Iteration 210 return: -1144.0541549236625\n",
            "Iteration 211 return: -1155.5939519602184\n",
            "Iteration 212 return: -1199.511068143382\n",
            "Iteration 213 return: -1139.1637171405764\n",
            "Iteration 214 return: -1165.1007731187344\n",
            "Iteration 215 return: -1163.5418150883634\n",
            "Iteration 216 return: -1140.1264750001824\n",
            "Iteration 217 return: -1217.4795380913172\n",
            "Iteration 218 return: -1175.4367359772884\n",
            "Iteration 219 return: -1152.4690969802332\n",
            "Iteration 220 return: -1173.535078179051\n",
            "Iteration 221 return: -1171.7303664351539\n",
            "Iteration 222 return: -1181.374667465578\n",
            "Iteration 223 return: -1145.6322062826684\n",
            "Iteration 224 return: -1158.24943132047\n",
            "Iteration 225 return: -1156.6781107185218\n",
            "Iteration 226 return: -1161.5123594859808\n",
            "Iteration 227 return: -1169.425703641716\n",
            "Iteration 228 return: -1178.9165999715021\n",
            "Iteration 229 return: -1186.7400059227432\n",
            "Iteration 230 return: -1196.050227307833\n",
            "Iteration 231 return: -1162.5001836688257\n",
            "Iteration 232 return: -1160.3827045159376\n",
            "Iteration 233 return: -1175.0753672380986\n",
            "Iteration 234 return: -1176.3046603425096\n",
            "Iteration 235 return: -1158.3073115538775\n",
            "Iteration 236 return: -1155.503804747869\n",
            "Iteration 237 return: -1133.589772520768\n",
            "Iteration 238 return: -1161.854816638565\n",
            "Iteration 239 return: -1171.1924935899767\n",
            "Iteration 240 return: -1155.9271470141346\n",
            "Iteration 241 return: -1172.829780895058\n",
            "Iteration 242 return: -1161.4821298467677\n",
            "Iteration 243 return: -1173.2107725270846\n",
            "Iteration 244 return: -1154.6958623574349\n",
            "Iteration 245 return: -1140.4850041453867\n",
            "Iteration 246 return: -1153.8725413327943\n",
            "Iteration 247 return: -1162.377844385774\n",
            "Iteration 248 return: -1167.3709230166635\n",
            "Iteration 249 return: -1162.805921832652\n",
            "Iteration 250 return: -1167.7028884121885\n",
            "Iteration 251 return: -1175.3337293752788\n",
            "Iteration 252 return: -1186.8003115043514\n",
            "Iteration 253 return: -1138.4824730277435\n",
            "Iteration 254 return: -1177.7472044414158\n",
            "Iteration 255 return: -1155.813925987105\n",
            "Iteration 256 return: -1146.165850391433\n",
            "Iteration 257 return: -1156.5703209793335\n",
            "Iteration 258 return: -1176.7697408477247\n",
            "Iteration 259 return: -1118.685946587574\n",
            "Iteration 260 return: -1167.8042733604511\n",
            "Iteration 261 return: -1152.3187599290154\n",
            "Iteration 262 return: -1152.265684527629\n",
            "Iteration 263 return: -1151.723072571971\n",
            "Iteration 264 return: -1157.0445750372794\n",
            "Iteration 265 return: -1138.269606210324\n",
            "Iteration 266 return: -1141.2474888396978\n",
            "Iteration 267 return: -1135.9532709329408\n",
            "Iteration 268 return: -1157.8768090891397\n",
            "Iteration 269 return: -1158.0202353700654\n",
            "Iteration 270 return: -1161.5303765597769\n",
            "Iteration 271 return: -1151.8112639195713\n",
            "Iteration 272 return: -1175.5766233794043\n",
            "Iteration 273 return: -1141.856276337523\n",
            "Iteration 274 return: -1151.9635749737763\n",
            "Iteration 275 return: -1132.7052733886553\n",
            "Iteration 276 return: -1153.3605800569849\n",
            "Iteration 277 return: -1136.116673815269\n",
            "Iteration 278 return: -1141.357568996714\n",
            "Iteration 279 return: -1149.9910297481824\n",
            "Iteration 280 return: -1104.904104174965\n",
            "Iteration 281 return: -1125.7031242702978\n",
            "Iteration 282 return: -1151.920218097532\n",
            "Iteration 283 return: -1131.129029990411\n",
            "Iteration 284 return: -1143.5060691873605\n",
            "Iteration 285 return: -1119.0131580842985\n",
            "Iteration 286 return: -1113.9455229403839\n",
            "Iteration 287 return: -1128.8913041973797\n",
            "Iteration 288 return: -1122.8145896658775\n",
            "Iteration 289 return: -1139.299235378375\n",
            "Iteration 290 return: -1133.2864119996973\n",
            "Iteration 291 return: -1135.2713608155632\n",
            "Iteration 292 return: -1137.526806675326\n",
            "Iteration 293 return: -1092.5357252891238\n",
            "Iteration 294 return: -1157.1623510226173\n",
            "Iteration 295 return: -1131.5243263084317\n",
            "Iteration 296 return: -1122.3157966823696\n",
            "Iteration 297 return: -1106.00656504421\n",
            "Iteration 298 return: -1116.0696764386996\n",
            "Iteration 299 return: -1099.5800126649233\n",
            "Iteration 300 return: -1109.2138243607824\n",
            "Iteration 301 return: -1111.7384331529067\n",
            "Iteration 302 return: -1104.5755215342776\n",
            "Iteration 303 return: -1096.2829832594127\n",
            "Iteration 304 return: -1127.4829812877533\n",
            "Iteration 305 return: -1131.1057390011063\n",
            "Iteration 306 return: -1112.7177414778798\n",
            "Iteration 307 return: -1102.6347023321205\n",
            "Iteration 308 return: -1119.4673943842374\n",
            "Iteration 309 return: -1118.6435425253985\n",
            "Iteration 310 return: -1098.634584777108\n",
            "Iteration 311 return: -1091.8799075764791\n",
            "Iteration 312 return: -1083.2653827762808\n",
            "Iteration 313 return: -1101.7446156955139\n",
            "Iteration 314 return: -1098.7419274427928\n",
            "Iteration 315 return: -1087.9506918550776\n",
            "Iteration 316 return: -1100.4311530604245\n",
            "Iteration 317 return: -1113.9775712038424\n",
            "Iteration 318 return: -1102.7557655976682\n",
            "Iteration 319 return: -1095.0348635932423\n",
            "Iteration 320 return: -1062.196891228188\n",
            "Iteration 321 return: -1065.1116430867348\n",
            "Iteration 322 return: -1075.759338640676\n",
            "Iteration 323 return: -1089.7310252635787\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F520Gtj4bl8J"
      },
      "source": [
        "# collect saved results\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "r1 = np.genfromtxt(\"InvertedPendulum_results.csv\", delimiter=\",\")\n",
        "all_results = [r1]\n",
        "labels = [\"REINFORCE\"]\n",
        "\n",
        "##############################################################\n",
        "# Plot your Policy Gradient results below\n",
        "##############################################################\n",
        "?"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}